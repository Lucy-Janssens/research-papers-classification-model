{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "train_data = pd.read_csv('./archive/train.csv')\n",
    "train_data.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate the proportion of each category\n",
    "category_proportions = train_data.iloc[:, 3:].mean()\n",
    "\n",
    "# Print the proportions to check the values\n",
    "print(category_proportions)\n",
    "print(train_data.columns)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=category_proportions.index, y=category_proportions.values)\n",
    "plt.title('Proportion of Samples in Each Category')\n",
    "plt.ylabel('Proportion')\n",
    "plt.xlabel('Categories')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 12\u001B[0m\n\u001B[1;32m      9\u001B[0m oov_tok \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m<OOV>\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# Combining TITLE and ABSTRACT for the input data\u001B[39;00m\n\u001B[0;32m---> 12\u001B[0m all_text \u001B[38;5;241m=\u001B[39m train_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTITLE\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m train_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mABSTRACT\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# Tokenization\u001B[39;00m\n\u001B[1;32m     15\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m Tokenizer(num_words\u001B[38;5;241m=\u001B[39mvocab_size, oov_token\u001B[38;5;241m=\u001B[39moov_tok)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Parameters (you can tune these)\n",
    "vocab_size = 10000\n",
    "max_length = 120\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "oov_tok = '<OOV>'\n",
    "\n",
    "# Combining TITLE and ABSTRACT for the input data\n",
    "all_text = train_data['TITLE'] + ' ' + train_data['ABSTRACT']\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(all_text)\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(all_text)\n",
    "padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "# Prepare target variables\n",
    "labels = train_data.iloc[:, 3:].values\n",
    "print(labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-05T13:20:02.214325Z",
     "start_time": "2023-12-05T13:20:02.206531Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# if on Apple Silicon M1\n",
    "from tensorflow.keras.optimizers.legacy import Adam"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# otherwise\n",
    "#from tensorflow.keras.optimizers import Adam"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense\n",
    "\n",
    "# Model parameters\n",
    "embedding_dim = 16\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(24, activation='relu'),\n",
    "    Dense(labels.shape[1], activation='sigmoid')  # Output layer with one neuron per category\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(padded, labels, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "epochs = 20\n",
    "batch_size = 64\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n",
    "                    validation_data=(X_val, y_val), verbose=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting training and validation accuracy\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting training and validation loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save the model\n",
    "model.save('research_paper_classification3.keras')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# evaluate the model in % of accuracy\n",
    "loss, accuracy = model.evaluate(X_val, y_val, verbose=2)\n",
    "\n",
    "# Print the accuracy\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Get binary predictions for the validation set\n",
    "val_predictions = (model.predict(X_val) > 0.5).astype(int)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_val, val_predictions, target_names=category_names))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Making Predictions on Test Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-05 15:19:31.907927: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2023-12-05 15:19:31.907956: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2023-12-05 15:19:31.907961: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2023-12-05 15:19:31.907987: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-12-05 15:19:31.908001: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('research_paper_classification2.keras', compile=False) # model with the best accuracy 77.33%\n",
    "model.compile()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-05T13:19:32.547640Z",
     "start_time": "2023-12-05T13:19:29.246526Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the test dataset\n",
    "test_data = pd.read_csv('archive/test.csv')\n",
    "\n",
    "# Combining TITLE and ABSTRACT for the input data\n",
    "test_text = test_data['TITLE'] + ' ' + test_data['ABSTRACT']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-05T13:19:35.940683Z",
     "start_time": "2023-12-05T13:19:35.854368Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 6\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocessing\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtext\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Tokenizer\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# Preprocess the test data\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m test_sequences \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39mtexts_to_sequences(test_text)\n\u001B[1;32m      7\u001B[0m test_padded \u001B[38;5;241m=\u001B[39m pad_sequences(test_sequences, maxlen\u001B[38;5;241m=\u001B[39mmax_length, padding\u001B[38;5;241m=\u001B[39mpadding_type, truncating\u001B[38;5;241m=\u001B[39mtrunc_type)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Preprocess the test data\n",
    "test_sequences = tokenizer.texts_to_sequences(test_text)\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-05T13:20:05.380885Z",
     "start_time": "2023-12-05T13:20:05.375289Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_padded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Make predictions\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m predictions \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict(test_padded)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'test_padded' is not defined"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions = model.predict(test_padded)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-05T13:20:13.822069Z",
     "start_time": "2023-12-05T13:20:13.816538Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Applying a threshold to convert probabilities to binary values\n",
    "threshold = 0.5\n",
    "number_of_papers = 10\n",
    "binary_predictions = (predictions > threshold).astype(int)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Displaying the first few binary predictions\n",
    "print(binary_predictions[:number_of_papers])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Assuming you have a list of category names corresponding to the order in your label columns\n",
    "category_names = ['Computer Science', 'Physics', 'Mathematics', 'Statistics', 'Quantitative Biology', 'Quantitative Finance']\n",
    "\n",
    "# Function to map binary predictions to category names\n",
    "def map_predictions(binary_preds, category_names):\n",
    "    pred_labels = []\n",
    "    for pred in binary_preds:\n",
    "        labels = [category_names[i] for i, val in enumerate(pred) if val == 1]\n",
    "        pred_labels.append(labels)\n",
    "    return pred_labels\n",
    "\n",
    "# Mapping the first few predictions\n",
    "mapped_predictions = map_predictions(binary_predictions[:number_of_papers], category_names)\n",
    "\n",
    "# Display mapped predictions\n",
    "for i, pred in enumerate(mapped_predictions):\n",
    "    print(f\"Test Sample {i+1}: {pred if pred else 'No Categories Predicted'}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Display predictions with test data for the first few samples\n",
    "for i in range(number_of_papers):\n",
    "    print(f\"Test Sample {i+1}:\")\n",
    "    print(f\"Title: {test_data['TITLE'].iloc[i]}\")\n",
    "    print(f\"Abstract: {test_data['ABSTRACT'].iloc[i]}\")\n",
    "    print(f\"Predicted Categories: {mapped_predictions[i] if mapped_predictions[i] else 'No Categories Predicted'}\")\n",
    "    print(\"\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save predictions in a dataframe\n",
    "submission_df = pd.DataFrame(columns=['ID'] + category_names)\n",
    "submission_df['ID'] = test_data['ID'].values\n",
    "# add title and abstract columns\n",
    "submission_df['TITLE'] = test_data['TITLE'].values\n",
    "submission_df['ABSTRACT'] = test_data['ABSTRACT'].values\n",
    "# add predictions\n",
    "submission_df[category_names] = binary_predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save the dataframe as excel file\n",
    "submission_df.to_excel('submission.xlsx', index=False)\n",
    "# display the dataframe with predictions\n",
    "submission_df.head(20)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
